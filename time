import argparse
import os
import time
import warnings
from pathlib import Path
import torch
from pytorch_lightning import seed_everything

from beat_this.dataset import BeatDataModule
from beat_this.inference import load_checkpoint
from beat_this.model.pl_module import PLBeatThis



seed_everything(0, workers=True)


def main(args):
    if not torch.cuda.is_available():
        raise RuntimeError(" CUDA GPU not found — GPU required for this test.")

    torch.cuda.set_device(args.gpu)
    device = torch.device("cuda", args.gpu)
    print(f"\n Using GPU: {torch.cuda.get_device_name(device)} (id={args.gpu})")

    # --- Load checkpoint & data ---
    checkpoint = load_checkpoint(args.models[0])
    datamodule = datamodule_setup(checkpoint, args.num_workers, args.datasplit)
    dataloader = datamodule.predict_dataloader()

    # --- Build and load model ---
    model = plmodel_setup(checkpoint, args.eval_trim_beats, args.dbn).to(device)
    model.eval()

    # --- Warm-up GPU (2-3 batches) ---
    warmup(model, dataloader, device, num_batches=2)

    # --- Measure forward-pass time only ---
    total_time, avg_batch_time = measure_model_time(model, dataloader, device, args.max_batches)

    print("\n==============================")
    print(" Pure Model Inference Timing")
    print("==============================")
    print(f"Total forward time: {total_time:.3f} s")
    print(f"Average per batch: {avg_batch_time * 1000:.3f} ms")
    print("==============================")


def datamodule_setup(checkpoint, num_workers, datasplit):
    data_dir = Path(__file__).parent.parent / "data"
    params = checkpoint["datamodule_hyper_parameters"].copy()
    params["num_workers"] = num_workers or 0
    params["predict_datasplit"] = datasplit
    params["data_dir"] = data_dir
    params["logits_dir"] = data_dir / "output_npy_files"

    dm = BeatDataModule(**params)
    dm.setup(stage="fit")
    dm.setup(stage="predict")
    return dm


def plmodel_setup(checkpoint, eval_trim_beats, dbn):
    if eval_trim_beats is not None:
        checkpoint["hyper_parameters"]["eval_trim_beats"] = eval_trim_beats
    if dbn is not None:
        checkpoint["hyper_parameters"]["use_dbn"] = dbn
    model = PLBeatThis(**checkpoint["hyper_parameters"])
    model.load_state_dict(checkpoint["state_dict"], strict=False)
    model.cuda()
    return model


@torch.inference_mode()
def warmup(model, dataloader, device, num_batches=2):
    print("\n Warming up GPU...")
    it = iter(dataloader)
    for i in range(num_batches):
        try:
            batch = next(it)
        except StopIteration:
            break
        batch = move_to_device(batch, device)
        with torch.amp.autocast("cuda", dtype=torch.float16):
            _ = model.predict_step(batch, i)
    torch.cuda.synchronize()


@torch.inference_mode()
def measure_model_time(model, dataloader, device, max_batches=None):
    print("\n Measuring pure model forward-pass time (GPU only)...")
    total_time = 0.0
    num_batches = 0

    for i, batch in enumerate(dataloader):
        if max_batches and i >= max_batches:
            break

        batch = move_to_device(batch, device)

        torch.cuda.synchronize()
        start = time.perf_counter()

        # ✅ Measure only model forward pass (no metrics or postproc)
        with torch.amp.autocast("cuda", dtype=torch.float16):
            _ = model.predict_step(batch, i)

        torch.cuda.synchronize()
        end = time.perf_counter()

        total_time += (end - start)
        num_batches += 1

    avg_time = total_time / num_batches if num_batches else 0
    return total_time, avg_time


def move_to_device(batch, device):
    if isinstance(batch, torch.Tensor):
        return batch.to(device, non_blocking=True)
    elif isinstance(batch, dict):
        return {k: move_to_device(v, device) for k, v in batch.items()}
    elif isinstance(batch, (list, tuple)):
        return [move_to_device(v, device) for v in batch]
    return batch


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Measure GPU model-only inference time.")
    parser.add_argument("--models", type=str, nargs="+", required=True, help="Checkpoint path(s)")
    parser.add_argument("--datasplit", type=str, choices=("train", "val", "test"), default="test")
    parser.add_argument("--gpu", type=int, default=0)
    parser.add_argument("--num_workers", type=int, default=0)
    parser.add_argument("--eval_trim_beats", type=float, default=None)
    parser.add_argument("--dbn", default=None, action=argparse.BooleanOptionalAction)
    parser.add_argument("--max-batches", type=int, default=None, help="Limit batches to test (optional)")
    args = parser.parse_args()
    main(args)
